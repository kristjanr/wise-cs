{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-25T18:48:30.168051Z",
     "start_time": "2023-06-25T18:48:30.112682Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Replace newline characters with spaces\n",
    "    cleaned_text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    cleaned_text = \" \".join(cleaned_text.split())\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def parse_section(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    buttons = soup.find_all('button')\n",
    "    accordion_contents = soup.find_all('div', {'data-testid': lambda x: x and x.startswith('accordion-content-')})\n",
    "    parsed_data = []\n",
    "    for i, (button, content) in enumerate(zip(buttons, accordion_contents), 1):\n",
    "        heading = button.find('h2').text\n",
    "        articles = []\n",
    "        list_items = content.find_all('li')\n",
    "        for j, li in enumerate(list_items, start=1):\n",
    "            a = li.find('a')\n",
    "            title = a.text\n",
    "            link = a['href']\n",
    "            articles.append(dict(title=clean_text(title), link=link, order=j))\n",
    "        parsed_data.append(dict(order=i, heading=clean_text(heading), articles=articles))\n",
    "    return parsed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import os\n",
    "\n",
    "data = []\n",
    "\n",
    "for section_folder_name in ['01-sending-money', '02-managing-your-account', '03-holding-money', '04-wise-card',\n",
    "                            '05-receiving-money', '06-wise-business']:\n",
    "    source_path = 'scraped-data/sections/' + section_folder_name + '/source.py'\n",
    "    module_name = os.path.basename(source_path).split('.')[0]\n",
    "    spec = importlib.util.spec_from_file_location(module_name, source_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    path = '/'.join(source_path.split('/')[:3])\n",
    "    order_str = source_path.split('/')[2][:2]\n",
    "    data.append(dict(path=path, order=int(order_str), link=module.link, heading=module.heading, title=module.title,\n",
    "                     subsections=parse_section(module.html)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T18:37:35.493891Z",
     "start_time": "2023-06-25T18:37:35.459880Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "\"{'path': 'scraped-data/sections/01-sending-money', 'order': 1, 'link': 'https://wise.com/help/topics/5bVKT0uQdBrDp6T62keyfz/sending-money', 'heading': 'Sending money', 'title': 'Setting up, paying for, editing, and cancelling transfers.', 'subsections': [{'order': 1, 'heading': 'Sending money basics', 'articles': [{'title': 'How do I send money with Wise?', 'link': '/help/articles/2977959/how-do-i-send-money-with-wise', 'order': 1}, {'title': 'How long does a transfer take?', 'link': '/help/arti\""
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(data[0])[:500]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T18:37:36.369256Z",
     "start_time": "2023-06-25T18:37:36.362480Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "\"{'order': 1, 'heading': 'Sending money basics', 'articles': [{'title': 'How do I send money with Wise?', 'link': '/help/articles/2977959/how-do-i-send-money-with-wise', 'order': 1}, {'title': 'How long does a transfer take?', 'link': '/help/articles/2524078/how-long-does-a-transfer-take', 'order': 2}, {'title': 'Can I send exact amounts?', 'link': '/help/articles/2448314/can-i-send-exact-amounts', 'order': 3}, {'title': 'How do you notify me about a transfer?', 'link': '/help/articles/2553293/ho\""
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(data[0]['subsections'][0])[:500]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T18:37:36.769405Z",
     "start_time": "2023-06-25T18:37:36.765140Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "{'title': 'How do I send money with Wise?',\n 'link': '/help/articles/2977959/how-do-i-send-money-with-wise',\n 'order': 1}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['subsections'][0]['articles'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T18:37:37.125705Z",
     "start_time": "2023-06-25T18:37:37.120839Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# sum the number of articles in each content\n",
    "sanity_check = {d['order']: sum([len(subsection['articles']) for subsection in d['subsections']]) for d in data}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T18:37:37.523800Z",
     "start_time": "2023-06-25T18:37:37.518388Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "289"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of articles in total\n",
    "sum(sanity_check.values())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T18:37:38.582544Z",
     "start_time": "2023-06-25T18:37:38.576681Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "import requests\n",
    "import html2text\n",
    "\n",
    "# Convert HTML content to Markdown\n",
    "converter = html2text.HTML2Text()\n",
    "# Don't want to deal with inline links\n",
    "converter.ignore_links = True\n",
    "# Ignore images\n",
    "converter.ignore_images = True\n",
    "# Ignore tables\n",
    "converter.ignore_tables = True\n",
    "converter.body_width = 0  # Disable line wrapping\n",
    "\n",
    "base_url = \"https://wise.com\"\n",
    "\n",
    "\n",
    "def get_article_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    html = soup.find(\"div\", {\"class\": \"article-content\"})\n",
    "    if not html:\n",
    "        print(f\"Could not find article content in {url}\")\n",
    "        return None, None, None\n",
    "    related_articles = []\n",
    "    related_articles_section = soup.find(\"ul\", {\"class\": \"css-1mcz8c5\"})\n",
    "    if related_articles_section:\n",
    "        for li in related_articles_section.find_all(\"li\"):\n",
    "            a = li.find(\"a\")\n",
    "            related_articles.append({\n",
    "                \"title\": a.get_text().strip(),\n",
    "                \"link\": base_url + a[\"href\"]\n",
    "            })\n",
    "\n",
    "    # Convert tables to CSV and replace them with markers\n",
    "    tables = html.find_all(\"table\")\n",
    "    csv_tables = []\n",
    "    if tables:\n",
    "        print(f'Found {len(tables)} tables in {url}')\n",
    "    for i, table in enumerate(tables):\n",
    "        table_csv = StringIO()\n",
    "        csv_writer = csv.writer(table_csv, lineterminator='\\n')\n",
    "\n",
    "        rows = table.find_all(\"tr\")\n",
    "        for row in rows:\n",
    "            cells = row.find_all([\"th\", \"td\"])\n",
    "            cell_list = [cell.get_text().strip() for cell in cells]\n",
    "            csv_writer.writerow(cell_list)\n",
    "        table_csv_getvalue = table_csv.getvalue().replace(\"\\n\", \"  \\n\")\n",
    "        csv_tables.append(table_csv_getvalue)\n",
    "        table.replace_with(f\"CSV_TABLE_MARKER_{i}\")  # Place a marker\n",
    "\n",
    "    # Convert HTML to markdown\n",
    "    markdown_content = converter.handle(str(html))\n",
    "\n",
    "    # Replace markers with CSV tables\n",
    "    for i, table_csv_str in enumerate(csv_tables):\n",
    "        markdown_content = markdown_content.replace(f\"CSV_TABLE_MARKER_{i}\",\n",
    "                                                    \"\\n--- CSV table begins ---\" + \"  \\n\" + table_csv_str + \"--- CSV table ends ---  \\n\")\n",
    "\n",
    "    return html, markdown_content.strip(), related_articles\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T22:34:24.350455Z",
     "start_time": "2023-06-25T22:34:24.348902Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m section \u001B[38;5;129;01min\u001B[39;00m data:\n\u001B[1;32m      5\u001B[0m     path_subsections_ \u001B[38;5;241m=\u001B[39m section[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/subsections/\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 6\u001B[0m     \u001B[43mos\u001B[49m\u001B[38;5;241m.\u001B[39mmakedirs(path_subsections_, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m subsection \u001B[38;5;129;01min\u001B[39;00m section[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msubsections\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[1;32m      8\u001B[0m         subsection_folder \u001B[38;5;241m=\u001B[39m path_subsections_ \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(subsection[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124morder\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mzfill(\u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m slugify(subsection[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mheading\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "from slugify import slugify\n",
    "import json\n",
    "\n",
    "for section in data:\n",
    "    path_subsections_ = section['path'] + '/subsections/'\n",
    "    os.makedirs(path_subsections_, exist_ok=True)\n",
    "    for subsection in section['subsections']:\n",
    "        subsection_folder = path_subsections_ + str(subsection['order']).zfill(2) + '-' + slugify(subsection['heading'])\n",
    "        os.makedirs(subsection_folder, exist_ok=True)\n",
    "        subsection['folder_path'] = subsection_folder\n",
    "        os.makedirs(subsection_folder + '/articles', exist_ok=True)\n",
    "        for article in subsection['articles']:\n",
    "            article_folder_name = slugify(article['title'])\n",
    "            article_folder_path = subsection_folder + '/articles/' + str(article['order']).zfill(\n",
    "                2) + '-' + article_folder_name\n",
    "            # add article_folder_path to article\n",
    "            article['folder_path'] = article_folder_path\n",
    "            os.makedirs(article_folder_path, exist_ok=True)\n",
    "            article_url = base_url + article[\"link\"]\n",
    "            print(f'Fetching {article_url} and saving to {article_folder_path}')\n",
    "            html, md, related_articles = get_article_content(article_url)\n",
    "            md_with_headings = f\"# {subsection['heading']}  \\n## {article['title']}  \\n{md}\"\n",
    "            with open(article_folder_path + '/' + 'content.md', 'w') as f:\n",
    "                f.write(md_with_headings)\n",
    "            with open(article_folder_path + '/' + 'metadata.json', 'w') as f:\n",
    "                metadata = dict(title=article['title'], link=article_url, related_articles=related_articles)\n",
    "                json.dump(metadata, f, indent=4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T22:20:19.417692Z",
     "start_time": "2023-06-25T22:20:19.399900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjson\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscraped-data/index.json\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m----> 5\u001B[0m     json\u001B[38;5;241m.\u001B[39mdump(\u001B[43mdata\u001B[49m, f, indent\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "## serialize data\n",
    "import json\n",
    "\n",
    "with open('scraped-data/index.json', 'w') as f:\n",
    "    json.dump(data, f, indent=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T22:17:52.389772Z",
     "start_time": "2023-06-25T22:17:52.289197Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"scraped-data/index.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T22:20:00.538728Z",
     "start_time": "2023-06-25T22:20:00.532431Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "288"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_articles = set()\n",
    "for section in data:\n",
    "    for subsection in section[\"subsections\"]:\n",
    "        for article in subsection[\"articles\"]:\n",
    "            article_url = base_url + article[\"link\"]\n",
    "            existing_articles.add(article_url)\n",
    "len(existing_articles)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T23:17:03.855671Z",
     "start_time": "2023-06-25T23:17:03.851876Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# walk through all folders and subfolders at /Users/kristjan.roosild/repos/wise-cs/scraped-data/sections and find all files called metadata.json\n",
    "# open them and read the related_articles list. Each element in the list is a dict which has a link key. Add all the links to the  related_articles_not_scraped list\n",
    "import os\n",
    "related_articles = dict()\n",
    "\n",
    "for root, dirs, files in os.walk('scraped-data/sections'):\n",
    "    for file in files:\n",
    "        if file == 'metadata.json':\n",
    "            with open(os.path.join(root, file)) as f:\n",
    "                metadata = json.load(f)\n",
    "                for related_article in metadata['related_articles']:\n",
    "                    # drop the query string\n",
    "                    cleaned_up_url = related_article['link'].split('?')[0]\n",
    "                    if cleaned_up_url not in existing_articles:\n",
    "                        related_articles[cleaned_up_url] = related_article['title']\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T23:17:30.421652Z",
     "start_time": "2023-06-25T23:17:30.374158Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "146"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(related_articles)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T23:17:33.210340Z",
     "start_time": "2023-06-25T23:17:33.203980Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# scrape the remaining articles\n",
    "os.makedirs('scraped-data/related-articles/', exist_ok=True)\n",
    "\n",
    "\n",
    "def scrape_related(related_articles_not_scraped):\n",
    "    for article_url, article_title_ in related_articles_not_scraped.items():\n",
    "        article_folder_name = slugify(article_title_)\n",
    "        print(f'Fetching {article_url}')\n",
    "        try:\n",
    "            html, md, related_articles = get_article_content(article_url)\n",
    "        except AttributeError:\n",
    "            raise Exception(f'Failed to fetch {article_url}')\n",
    "        if not html:\n",
    "            continue\n",
    "        full_folder_path = 'scraped-data/related-articles/' + article_folder_name\n",
    "        print(f'Saving to {full_folder_path}')\n",
    "        os.makedirs(full_folder_path, exist_ok=True)\n",
    "        md_with_headings = f\"## {article_title_}  \\n{md}\"\n",
    "        with open(full_folder_path + '/' + 'content.md', 'w') as f:\n",
    "            f.write(md_with_headings)\n",
    "        with open(full_folder_path + '/' + 'metadata.json', 'w') as f:\n",
    "            metadata = dict(title=article_title_, link=article_url, related_articles=related_articles)\n",
    "            json.dump(metadata, f, indent=4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T23:17:38.983560Z",
     "start_time": "2023-06-25T23:17:38.977609Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "existing_related_articles = set()\n",
    "for root, dirs, files in os.walk('scraped-data/related-articles'):\n",
    "    for file in files:\n",
    "        if file == 'metadata.json':\n",
    "            with open(os.path.join(root, file)) as f:\n",
    "                # check if file is empty\n",
    "                if os.stat(os.path.join(root, file)).st_size == 0:\n",
    "                    continue\n",
    "                metadata = json.load(f)\n",
    "                article_url = metadata['link']\n",
    "                existing_related_articles.add(article_url)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T23:18:03.197239Z",
     "start_time": "2023-06-25T23:18:03.125135Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "6"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "related_articles_not_scraped = dict()\n",
    "for root, dirs, files in os.walk('scraped-data/related-articles'):\n",
    "    for file in files:\n",
    "        if file == 'metadata.json':\n",
    "            with open(os.path.join(root, file)) as f:\n",
    "                # check if file is empty\n",
    "                if os.stat(os.path.join(root, file)).st_size == 0:\n",
    "                    continue\n",
    "                metadata = json.load(f)\n",
    "                for related_article in metadata['related_articles']:\n",
    "                    # drop the query string\n",
    "                    cleaned_up_url = related_article['link'].split('?')[0]\n",
    "                    if cleaned_up_url not in existing_related_articles:\n",
    "                        related_articles_not_scraped[cleaned_up_url] = related_article['title']\n",
    "len(related_articles_not_scraped)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T23:18:19.953038Z",
     "start_time": "2023-06-25T23:18:19.906158Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://wise.com/help/articles/PhxIBARqUVe4P7yZNVxZw/how-do-i-approvereject-a-payment-requiring-approval\n",
      "Saving to scraped-data/related-articles/how-do-i-approve-reject-a-payment-requiring-approval\n",
      "Fetching https://wise.com/help/articles/5dLOaJKVT5yAtq7G2OCCXS/how-do-i-check-the-details-of-an-approvedrejected-transfer\n",
      "Saving to scraped-data/related-articles/how-do-i-check-the-details-of-an-approved-rejected-transfer\n",
      "Fetching https://wise.com/help/articles/2958229/whats-an-api-token\n",
      "Could not find article content in https://wise.com/help/articles/2958229/whats-an-api-token\n",
      "Fetching https://wise.com/help/articles/6cskaMU5RKpsmxqN7n8X1C/how-does-my-connection-with-quickbooks-work\n",
      "Saving to scraped-data/related-articles/how-does-my-connection-with-quickbooks-work\n",
      "Fetching https://wise.com/help/articles/2932350/guide-to-gel-transfers\n",
      "Saving to scraped-data/related-articles/guide-to-gel-transfers\n",
      "Fetching https://wise.com/help/articles/6PgFe27V7Fw8v38Hygt4H0/earning-cash-back-with-your-wise-card\n",
      "Could not find article content in https://wise.com/help/articles/6PgFe27V7Fw8v38Hygt4H0/earning-cash-back-with-your-wise-card\n"
     ]
    }
   ],
   "source": [
    "scrape_related(related_articles_not_scraped)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T23:18:28.056033Z",
     "start_time": "2023-06-25T23:18:26.877554Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "notebooks",
   "language": "python",
   "display_name": "notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
